{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeniusTopics - WorkFlow\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The first part of the process requires loading the dataset and garnering a better understanding of it. To do this, I will use the `pandas` library to import the dataset .csv file as a `pandas.DataFrame` object. I will then use the `.head()` method to view the first five records, then the `.info()` method to understand the values held within the `DataFrame` better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(\"song_lyrics_subset_10000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>language_cld3</th>\n",
       "      <th>language_ft</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\r\\nKilla Cam, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Produced by Irv Gotti]\\r\\n\\r\\n[Intro]\\r\\nYeah...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>4743</td>\n",
       "      <td>{}</td>\n",
       "      <td>Maybe cause I'm eatin\\r\\nAnd these bastards fi...</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>144404</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}</td>\n",
       "      <td>[Produced by Kanye West and Brian Miller]\\r\\n\\...</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>78271</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\r\\nSo they ask me\\r\\n\"Young boy\\r\\nWha...</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title  tag     artist  year   views  \\\n",
       "0          Killa Cam  rap    Cam'ron  2004  173166   \n",
       "1         Can I Live  rap      JAY-Z  1996  468624   \n",
       "2  Forgive Me Father  rap   Fabolous  2003    4743   \n",
       "3       Down and Out  rap    Cam'ron  2004  144404   \n",
       "4             Fly In  rap  Lil Wayne  2005   78271   \n",
       "\n",
       "                                       features  \\\n",
       "0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "1                                            {}   \n",
       "2                                            {}   \n",
       "3  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n",
       "4                                            {}   \n",
       "\n",
       "                                              lyrics  id language_cld3  \\\n",
       "0  [Chorus: Opera Steve & Cam'ron]\\r\\nKilla Cam, ...   1            en   \n",
       "1  [Produced by Irv Gotti]\\r\\n\\r\\n[Intro]\\r\\nYeah...   3            en   \n",
       "2  Maybe cause I'm eatin\\r\\nAnd these bastards fi...   4            en   \n",
       "3  [Produced by Kanye West and Brian Miller]\\r\\n\\...   5            en   \n",
       "4  [Intro]\\r\\nSo they ask me\\r\\n\"Young boy\\r\\nWha...   6            en   \n",
       "\n",
       "  language_ft language  \n",
       "0          en       en  \n",
       "1          en       en  \n",
       "2          en       en  \n",
       "3          en       en  \n",
       "4          en       en  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   title          39999 non-null  object\n",
      " 1   tag            40000 non-null  object\n",
      " 2   artist         40000 non-null  object\n",
      " 3   year           40000 non-null  int64 \n",
      " 4   views          40000 non-null  int64 \n",
      " 5   features       40000 non-null  object\n",
      " 6   lyrics         40000 non-null  object\n",
      " 7   id             40000 non-null  int64 \n",
      " 8   language_cld3  40000 non-null  object\n",
      " 9   language_ft    40000 non-null  object\n",
      " 10  language       40000 non-null  object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# View the data information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Most columns appear to be strings, with the exception of `id`, `year` and `views`. These appear to be integers.\n",
    "- One of the rows does not have a value in the `title` column. This will be removed for consistency.\n",
    "- The `lyrics` column values appear to hold non-lyrical information within square brackets. These will need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Analysis\n",
    "\n",
    "Of interest to me personally, I think it would be interesting to identify the most commonly referring themes between each genre of music. This could be useful in a variety of ways, including musical genre classification algorithms, by assigning a most-likely genre to a track, based on the underlying themes of the song.\n",
    "\n",
    "In order to conduct this analysis, the workflow must undergo several operations:\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "This will allow for a reliable further-processing and subsequent analysis of the data, by cleaning and normalizing it.\n",
    "\n",
    "In order to be conducive to topic modelling, the data will undergo these transformations:\n",
    "- Cleaning\n",
    "    - Strip values inside square brackets. This will remove strings denote where in the song structure the following lines belong. These maybe useful in the future, should more granular analysis be undertaken, but not currently necessary.\n",
    "    - Strip punctuation. This will remove non-alphanumeric characters from the lyrics.\n",
    "    - Strip white space. This will remove double-spaces, returns, tabs from the lyrics.\n",
    "    - Strip stop words. This will remove words within the lyrics that are not conducive to the task of topic modelling.\n",
    "- Normalization\n",
    "    - Remove blank values.\n",
    "    - Normalize case. This will ensure all equivalent alphanumeric characters (and words) are directly comparable.\n",
    "    - Lemmatization. This will transform words to their english 'root' word, therefore allowing words with similar or same definitions to be given a singular identifying word, thus making it easier to process and better to analyze in the task of topic modelling.\n",
    "    - Word tokenization. This will split each lyrics document inputted into a list of words.\n",
    "\n",
    "### Quantification\n",
    "\n",
    "Quantification will allow for information to be understood by the chosen model/s for analysis, by representing the lyrics numerically. This will ensure analysis can be conducted by most common ML models, to provide an objective topic model for each set of lyrics.\n",
    "\n",
    "In the process of retrieving the most common topics representative of each musical genre, I decided the *Term Frequency-Inverse Document Frequency* method of quantifying terms within the lyrics of each genre would be the most beneficial, when retrieving the importance of words over each genre-specific corpus.\n",
    "\n",
    "Instead of simply counting how often a term occurs within a corpus, it counts how often the term appears in each document *(term frequency)*, and how frequently it appears over the entire corpus given *(inverse document frequency)*. The term frequency is then multiplied by the inverse document frequency to give a final figure.\n",
    "\n",
    "By applying this figure to each term within a corpus, words which appear more generally across a corpus are penalized, while words which are document-specific (and thus indicative of a more relevant term, rather than a general one) are given a greater weighting when analyzed. By doing this, the topic model will conduct analysis on terms not based on their frequency, but on their relevancy to the genre as a whole. In other words, this will allow me to find the most *relevant* topics for each music genre, not just the most *common*.\n",
    "\n",
    "### Model\n",
    "\n",
    "The chosen model for task of topic modelling is a Latent Dirichlet Allocation (LDA) model. Using LDA to extract *n* topics from a given corpus is a common technique that will allow for an analysis of the themes across a genre of music. I will display the topic by its most relevant word, along with a number of words associated with said topic, in the form of a word cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will remove the column with the missing `title` value, using the `.dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 39999 entries, 0 to 39999\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   title          39999 non-null  object\n",
      " 1   tag            39999 non-null  object\n",
      " 2   artist         39999 non-null  object\n",
      " 3   year           39999 non-null  int64 \n",
      " 4   views          39999 non-null  int64 \n",
      " 5   features       39999 non-null  object\n",
      " 6   lyrics         39999 non-null  object\n",
      " 7   id             39999 non-null  int64 \n",
      " 8   language_cld3  39999 non-null  object\n",
      " 9   language_ft    39999 non-null  object\n",
      " 10  language       39999 non-null  object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Remove row with missing title value\n",
    "df.dropna(subset=[\"title\"], inplace=True)\n",
    "\n",
    "# View counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have removed the row with the missing `title` value, I will separate the dataset into lists of lyrics (documents) for each genre. To do this, I will use square-bracket notation to access the `tag` column, and create a new copy of each `DataFrame` for each of the desired `tag` values (genres). After this, I will create a `Series` object of the `lyrics` column within each genre, while using the `.to_list()` method to create a list of documents (lyrics) for each `tag`-split `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataframes into different tags\n",
    "rap_df = df[df['tag'] == 'rap']\n",
    "rb_df = df[df['tag'] == 'rb']\n",
    "pop_df = df[df['tag'] == 'pop']\n",
    "rock_df = df[df['tag'] == 'rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a list of lyrics from each data frame\n",
    "rap_docs = rap_df['lyrics'].to_list()\n",
    "rb_docs = rb_df['lyrics'].to_list()\n",
    "pop_docs = pop_df['lyrics'].to_list()\n",
    "rock_docs = rock_df['lyrics'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have separated the `df` into lists of lyrics for each genre, I will normalize the case of each document into lowercase. To do this, I will use the built-in `.casefold()` function, while iterating over each document in each list using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the lyrics\n",
    "rap_lower = [doc.casefold() for doc in rap_docs]\n",
    "rb_lower = [doc.casefold() for doc in rb_docs]\n",
    "pop_lower = [doc.casefold() for doc in pop_docs]\n",
    "rock_lower = [doc.casefold() for doc in rock_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will use `regex.sub()` to remove patterns of square brackets containing alphanumeric characters within each document. As identified earlier, these appear to hold non-lyrical information that may be detrimental to the analysis.\n",
    "\n",
    "*NOTE: I am using the `regex` library here, instead of `re`. `regex` allows for the required complexity to deal with any nested square brackets properly (should they occur). For the sake of brevity, `regex` will then be used for all subsequent regular expressions in the preprocessing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex\n",
    "import regex\n",
    "\n",
    "# Compile regex pattern for square brackets\n",
    "pattern = regex.compile(r\"\\[([^[\\]]*+(?:(?R)[^[\\]]*+)*)\\]\")\n",
    "\n",
    "# Remove square brackets\n",
    "rap_no_sq_brackets = [regex.sub(pattern, \"\", doc) for doc in rap_lower]\n",
    "rb_no_sq_brackets = [regex.sub(pattern, \"\", doc) for doc in rb_lower]\n",
    "pop_no_sq_brackets = [regex.sub(pattern, \"\", doc) for doc in pop_lower]\n",
    "rock_no_sq_brackets = [regex.sub(pattern, \"\", doc) for doc in rock_lower]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have removed the non-lyrical information within the square brackets, I will deal with all the other punctuation held within the documents. Using the same method, but a different pattern, I will use `regex.sub()` to remove punctuation from each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regex pattern for punctuation\n",
    "pattern = regex.compile(r\"[^\\w\\s]\")\n",
    "\n",
    "# Remove punctuation\n",
    "rap_no_punc = [regex.sub(pattern, \"\", doc) for doc in rap_no_sq_brackets]\n",
    "rb_no_punc = [regex.sub(pattern, \"\", doc) for doc in rb_no_sq_brackets]\n",
    "pop_no_punc = [regex.sub(pattern, \"\", doc) for doc in pop_no_sq_brackets]\n",
    "rock_no_punc = [regex.sub(pattern, \"\", doc) for doc in rock_no_sq_brackets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, I will be removing all white space greater than two characters long. This will help normalize the corpus by normalizing all white space to a single character. This is useful as the lyrics appear to have line breaks and other white space fairly regularly. Again, I will be using the `regex.sub()` method, with a pattern to remove white space over two characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regex pattern to strip white space\n",
    "pattern = regex.compile(r\"\\s{2,}\")\n",
    "\n",
    "# Strip white space\n",
    "rap_no_white_space = [regex.sub(pattern, \" \", doc) for doc in rap_no_punc]\n",
    "rb_no_white_space = [regex.sub(pattern, \" \", doc) for doc in rb_no_punc]\n",
    "pop_no_white_space = [regex.sub(pattern, \" \", doc) for doc in pop_no_punc]\n",
    "rock_no_white_space = [regex.sub(pattern, \" \", doc) for doc in rock_no_punc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the white space between words, I will remove white space at the beginning and end of each document, by using the in-build `strip()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip white space\n",
    "rap_stripped = [doc.strip() for doc in rap_no_white_space]\n",
    "rb_stripped = [doc.strip() for doc in rb_no_white_space]\n",
    "pop_stripped = [doc.strip() for doc in pop_no_white_space]\n",
    "rock_stripped = [doc.strip() for doc in rock_no_white_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "rap_tokenized = [word_tokenize(doc) for doc in rap_stripped]\n",
    "rb_tokenized = [word_tokenize(doc) for doc in rb_stripped]\n",
    "pop_tokenized = [word_tokenize(doc) for doc in pop_stripped]\n",
    "rock_tokenized = [word_tokenize(doc) for doc in rock_stripped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the documents are cleansed of unwanted characters, I will remove stop words from each document. This will ensure that a lot of words that do not provide much insight into the themes of each genre will be removed, allowing for greater emphasis to be placed on the words that will provide greater insight into the topics discussed.\n",
    "\n",
    "To do this, I will use the `NLTK` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Retrieve english stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Strip stopwords\n",
    "rap_no_stops = [[word for word in doc if word not in stop_words] for doc in rap_tokenized]\n",
    "rb_no_stops = [[word for word in doc if word not in stop_words] for doc in rb_tokenized]\n",
    "pop_no_stops = [[word for word in doc if word not in stop_words] for doc in pop_tokenized]\n",
    "rock_no_stops = [[word for word in doc if word not in stop_words] for doc in rock_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stop words, I am going to lemmatize the words. This will ensure words with a similar definition become a single word, so that the analysis can better identify recurring topics within each document and corpus. I will import the `WordNetLemmatizer` from the `NLTK` library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return word, most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap_tagged = [[get_part_of_speech(word) for word in doc] for doc in rap_no_stops]\n",
    "rb_tagged = [[get_part_of_speech(word) for word in doc] for doc in rb_no_stops]\n",
    "pop_tagged = [[get_part_of_speech(word) for word in doc] for doc in pop_no_stops]\n",
    "rock_tagged = [[get_part_of_speech(word) for word in doc] for doc in rock_no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize\n",
    "rap_lemmatized = [[lemmatizer.lemmatize(word, pos=tag) for word, tag in doc]for doc in rap_tagged]\n",
    "rb_lemmatized = [[lemmatizer.lemmatize(word, pos=tag) for word, tag in doc]for doc in rb_tagged]\n",
    "pop_lemmatized = [[lemmatizer.lemmatize(word, pos=tag) for word, tag in doc]for doc in pop_tagged]\n",
    "rock_lemmatized = [[lemmatizer.lemmatize(word, pos=tag) for word, tag in doc]for doc in rock_tagged]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
